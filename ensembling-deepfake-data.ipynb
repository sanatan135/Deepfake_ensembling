{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10473785,"sourceType":"datasetVersion","datasetId":6482454},{"sourceId":10765033,"sourceType":"datasetVersion","datasetId":6677712}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision import transforms, datasets\nfrom transformers import ViTForImageClassification\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:40:20.101038Z","iopub.execute_input":"2025-02-17T08:40:20.101333Z","iopub.status.idle":"2025-02-17T08:40:39.323066Z","shell.execute_reply.started":"2025-02-17T08:40:20.101311Z","shell.execute_reply":"2025-02-17T08:40:39.322342Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Block 2: Data Preparation and Transformation\ndata_dir = \"/kaggle/input/deepfake-image-detection\"\n\n# Define the image transformations (resize to 224x224, convert to tensor, and normalize)\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Create training and validation datasets using ImageFolder.\ntrain_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train-20250112T065955Z-001'), transform=transform)\nval_dataset = datasets.ImageFolder(os.path.join(data_dir, 'test-20250112T065939Z-001'), transform=transform)\n\n# Create DataLoaders for efficient batching and shuffling\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:40:39.324106Z","iopub.execute_input":"2025-02-17T08:40:39.324651Z","iopub.status.idle":"2025-02-17T08:40:41.245208Z","shell.execute_reply.started":"2025-02-17T08:40:39.324614Z","shell.execute_reply":"2025-02-17T08:40:41.244541Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:40:41.247117Z","iopub.execute_input":"2025-02-17T08:40:41.247348Z","iopub.status.idle":"2025-02-17T08:40:41.305549Z","shell.execute_reply.started":"2025-02-17T08:40:41.247329Z","shell.execute_reply":"2025-02-17T08:40:41.304823Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Block 3a: Load Pre-trained ViT Model from Local Files downloaded from hugging face.\nlocal_vit_path = \"/kaggle/input/pretrained-vit\"  \n\nmodel_vit = ViTForImageClassification.from_pretrained(\n    local_vit_path,\n    num_labels=2, \n    ignore_mismatched_sizes=True  # Allows the classifier head to reinitialize for 2 classes.\n)\n#  Explicitly replace the classifier head:\nmodel_vit.classifier = nn.Linear(model_vit.config.hidden_size, 2)\nmodel_vit = model_vit.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:40:41.306819Z","iopub.execute_input":"2025-02-17T08:40:41.307126Z","iopub.status.idle":"2025-02-17T08:40:45.037969Z","shell.execute_reply.started":"2025-02-17T08:40:41.307099Z","shell.execute_reply":"2025-02-17T08:40:45.037038Z"}},"outputs":[{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at /kaggle/input/pretrained-vit and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#Load a Pre-trained ResNet Model and Adjust for Binary Classification\nfrom torchvision.models import resnet18\n\nmodel_resnet = resnet18(pretrained=True)\n# Modify the final fully-connected layer for 2 classes.\nnum_ftrs = model_resnet.fc.in_features\nmodel_resnet.fc = nn.Linear(num_ftrs, 2)\nmodel_resnet = model_resnet.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:40:45.039044Z","iopub.execute_input":"2025-02-17T08:40:45.039360Z","iopub.status.idle":"2025-02-17T08:40:45.544956Z","shell.execute_reply.started":"2025-02-17T08:40:45.039330Z","shell.execute_reply":"2025-02-17T08:40:45.544036Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 218MB/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Defining a Training Function and then Training Each Model.\ndef train_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()  # Set model to training mode\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for images, labels in dataloader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        # For ViT, outputs are in outputs.logits; for ResNet, outputs are directly logits.\n        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * images.size(0)\n        _, preds = torch.max(logits, 1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n    return running_loss / total, correct / total\n\ncriterion = nn.CrossEntropyLoss()\noptimizer_vit = optim.AdamW(model_vit.parameters(), lr=2e-5)\noptimizer_resnet = optim.AdamW(model_resnet.parameters(), lr=2e-5)\n\nnum_epochs = 3  \nfor epoch in range(num_epochs):\n    vit_loss, vit_acc = train_epoch(model_vit, train_loader, optimizer_vit, criterion, device)\n    resnet_loss, resnet_acc = train_epoch(model_resnet, train_loader, optimizer_resnet, criterion, device)\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(f\"ViT -> Loss: {vit_loss:.4f}, Acc: {vit_acc:.4f}\")\n    print(f\"ResNet -> Loss: {resnet_loss:.4f}, Acc: {resnet_acc:.4f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:40:45.545826Z","iopub.execute_input":"2025-02-17T08:40:45.546120Z","iopub.status.idle":"2025-02-17T08:41:56.260727Z","shell.execute_reply.started":"2025-02-17T08:40:45.546090Z","shell.execute_reply":"2025-02-17T08:41:56.259770Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\nViT -> Loss: 0.5002, Acc: 0.9374\nResNet -> Loss: 1.2166, Acc: 0.0710\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/3\nViT -> Loss: 0.1840, Acc: 1.0000\nResNet -> Loss: 0.9427, Acc: 0.1190\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/3\nViT -> Loss: 0.0509, Acc: 1.0000\nResNet -> Loss: 0.7279, Acc: 0.4572\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Ensemble Prediction using a Voting Ensemble\n\ndef ensemble_predict(images):\n    # Setrting both models to evaluation mode\n    model_vit.eval()\n    model_resnet.eval()\n    with torch.no_grad():\n        # Get predictions from each model.\n        outputs_vit = model_vit(images)\n        outputs_resnet = model_resnet(images)\n        \n        # Retrieve logits; ViT returns outputs.logits\n        logits_vit = outputs_vit.logits if hasattr(outputs_vit, \"logits\") else outputs_vit\n        logits_resnet = outputs_resnet.logits if hasattr(outputs_resnet, \"logits\") else outputs_resnet\n        \n        # Convert logits to probabilities\n        probs_vit = F.softmax(logits_vit, dim=1)\n        probs_resnet = F.softmax(logits_resnet, dim=1)\n        \n        # Average the probabilities from both models\n        avg_probs = (probs_vit + probs_resnet) / 2.0\n        \n        # Final prediction is the class with the highest averaged probability\n        preds = torch.argmax(avg_probs, dim=1)\n    return preds\n\n# Evaluating the ensemble on the validation set\ndef evaluate_ensemble(val_loader, device):\n    total = 0\n    correct = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            preds = ensemble_predict(images)\n            total += labels.size(0)\n            correct += (preds == labels).sum().item()\n    accuracy = correct / total\n    return accuracy\n\nensemble_acc = evaluate_ensemble(val_loader, device)\nprint(f\"Ensemble Validation Accuracy: {ensemble_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:41:56.261634Z","iopub.execute_input":"2025-02-17T08:41:56.261952Z","iopub.status.idle":"2025-02-17T08:42:06.993436Z","shell.execute_reply.started":"2025-02-17T08:41:56.261923Z","shell.execute_reply":"2025-02-17T08:42:06.992533Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Ensemble Validation Accuracy: 1.0000\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:42:06.994376Z","iopub.execute_input":"2025-02-17T08:42:06.994705Z","iopub.status.idle":"2025-02-17T08:42:06.998585Z","shell.execute_reply.started":"2025-02-17T08:42:06.994674Z","shell.execute_reply":"2025-02-17T08:42:06.997832Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Function to Collect Predictions and True Labels from a DataLoader\ndef get_predictions_and_labels(model, data_loader, device):\n    model.eval()  # Set model to evaluation mode\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():  # No gradients needed during inference\n        for images, labels in data_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            \n            # For ViT, outputs are in outputs.logits; for ResNet, outputs are directly logits.\n            logits = outputs.logits if hasattr(outputs, \"logits\") else outputs\n            \n            # Get predicted class indices\n            _, preds = torch.max(logits, 1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    return all_preds, all_labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:42:07.000394Z","iopub.execute_input":"2025-02-17T08:42:07.000709Z","iopub.status.idle":"2025-02-17T08:42:07.020341Z","shell.execute_reply.started":"2025-02-17T08:42:07.000687Z","shell.execute_reply":"2025-02-17T08:42:07.019687Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Get Predictions and Labels for ViT and ResNet\nvit_preds, vit_labels = get_predictions_and_labels(model_vit, val_loader, device)\nresnet_preds, resnet_labels = get_predictions_and_labels(model_resnet, val_loader, device)\n\n# Printing classification reports for additional insights:\nprint(\"ViT Classification Report:\")\nprint(classification_report(vit_labels, vit_preds))\n\nprint(\"ResNet Classification Report:\")\nprint(classification_report(resnet_labels, resnet_preds))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:45:39.403055Z","iopub.execute_input":"2025-02-17T08:45:39.403370Z","iopub.status.idle":"2025-02-17T08:45:56.478834Z","shell.execute_reply.started":"2025-02-17T08:45:39.403347Z","shell.execute_reply":"2025-02-17T08:45:56.477643Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"ViT Classification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       499\n\n    accuracy                           1.00       499\n   macro avg       1.00      1.00      1.00       499\nweighted avg       1.00      1.00      1.00       499\n\nResNet Classification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      0.60      0.75       499\n           1       0.00      0.00      0.00         0\n\n    accuracy                           0.60       499\n   macro avg       0.50      0.30      0.37       499\nweighted avg       1.00      0.60      0.75       499\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Computing Confusion Matrices for Both Models\ncm_vit = confusion_matrix(vit_labels, vit_preds)\ncm_resnet = confusion_matrix(resnet_labels, resnet_preds)\n\n# Plotting the confusion matrices side-by-side for comparison.\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Confusion matrix for ViT\nsns.heatmap(cm_vit, annot=True, fmt='d', cmap='Blues', ax=axes[0])\naxes[0].set_title('ViT Confusion Matrix')\naxes[0].set_xlabel('Predicted Label')\naxes[0].set_ylabel('True Label')\n\n# Confusion matrix for ResNet\nsns.heatmap(cm_resnet, annot=True, fmt='d', cmap='Blues', ax=axes[1])\naxes[1].set_title('ResNet Confusion Matrix')\naxes[1].set_xlabel('Predicted Label')\naxes[1].set_ylabel('True Label')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:38:19.934408Z","iopub.execute_input":"2025-02-17T09:38:19.934697Z","iopub.status.idle":"2025-02-17T09:38:20.017529Z","shell.execute_reply.started":"2025-02-17T09:38:19.934661Z","shell.execute_reply":"2025-02-17T09:38:20.016440Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b78a118db636>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compute Confusion Matrices for Both Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcm_vit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvit_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvit_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcm_resnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Plotting the confusion matrices side-by-side for comparison.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"],"ename":"NameError","evalue":"name 'confusion_matrix' is not defined","output_type":"error"}],"execution_count":1}]}